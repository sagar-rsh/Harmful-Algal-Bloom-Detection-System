{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35650a4-9a1c-4c4b-9e4f-e40e38670af6",
   "metadata": {},
   "source": [
    "## pip install pandas numpy matplotlib scipy earthaccess netCDF4 h5py pyproj opencv-python pillow jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60dcff-c32c-4842-9ad4-891ed7ab6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import earthaccess\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import ConvexHull\n",
    "import pickle\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "import h5py\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee473154-7192-4dce-94f6-6c46ef3888e5",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4560774-4b19-46b1-80cc-5ae8865ff3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATACUBE_CONFIG = {\n",
    "    'spatial_extent_km': 100,      #100km like the habnet\n",
    "    'temporal_extent_days': 10, \n",
    "    'spatial_resolution_km': 2,    # 2km bins for datacube\n",
    "    'temporal_resolution_days': 1 \n",
    "}\n",
    "\n",
    "# modis-aqua modalities for now\n",
    "HABNET_MODIS_AQUA_MODALITIES = [\n",
    "    'chlor_a',      \n",
    "    'Rrs_412',     \n",
    "    'Rrs_443',      \n",
    "    'Rrs_488',      \n",
    "    'Rrs_531',    \n",
    "    'Rrs_555',      \n",
    "    'par'           \n",
    "]\n",
    "\n",
    "# gulf of Mexico bounds\n",
    "GULF_BOUNDS = {\n",
    "    'lat_min': 24.0, 'lat_max': 30.5,\n",
    "    'lon_min': -88.0, 'lon_max': -80.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927fed6-9f8f-4c94-94b3-4df74101e7f4",
   "metadata": {},
   "source": [
    "### Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c3e37-2e5b-47bd-aa3c-9ee2ac66511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    base_dir = Path(\"habnet_datacube_data\")\n",
    "    raw_dir = base_dir / \"raw_modis_l2\"\n",
    "    processed_dir = base_dir / \"processed_h5_datacubes\"  # h5 format\n",
    "    \n",
    "    for directory in [base_dir, raw_dir, processed_dir]:\n",
    "        directory.mkdir(exist_ok=True)\n",
    "        \n",
    "    return base_dir, raw_dir, processed_dir\n",
    "\n",
    "# spatial bounds 100km\n",
    "def calculate_spatial_bounds(lat, lon, extent_km=100):\n",
    "    extent_deg = extent_km / 111.0  # 1 degree around 111 km\n",
    "    return {\n",
    "        'lat_min': lat - extent_deg/2,\n",
    "        'lat_max': lat + extent_deg/2,\n",
    "        'lon_min': lon - extent_deg/2,\n",
    "        'lon_max': lon + extent_deg/2\n",
    "    }\n",
    "\n",
    "\n",
    "# temporal bounds 10 days total\n",
    "def calculate_temporal_bounds(event_date, days_before=9):\n",
    "    end_date = event_date + timedelta(days=1)  \n",
    "    start_date = event_date - timedelta(days=days_before)\n",
    "    return {'start_date': start_date, 'end_date': end_date}\n",
    "\n",
    "# utm from coords (like habnet)\n",
    "def get_utm_zone_from_coords(lat, lon):\n",
    "    utm_zone = int((lon + 180) / 6) + 1\n",
    "    if lat >= 0:\n",
    "        epsg_code = f\"326{utm_zone:02d}\"  # northern hemisphere\n",
    "    else:\n",
    "        epsg_code = f\"327{utm_zone:02d}\"  # southern hemisphere\n",
    "    return epsg_code\n",
    "\n",
    "# utm projection (like habnet)\n",
    "def setup_utm_projection(lat, lon):\n",
    "    epsg_code = get_utm_zone_from_coords(lat, lon)\n",
    "    transformer_to_utm = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg_code}\", always_xy=True)\n",
    "    transformer_from_utm = Transformer.from_crs(f\"EPSG:{epsg_code}\", \"EPSG:4326\", always_xy=True)\n",
    "    return transformer_to_utm, transformer_from_utm, epsg_code\n",
    "\n",
    "\n",
    "def setup_nasa_earthdata():\n",
    "    print(\"Setting up NASA Earthdata auth\")\n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        if auth:\n",
    "            print(\" - NASA Earthdata auth successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\" - Auth failed\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"  Authentication error: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbbbf5-a783-4f18-a62a-bafed8c62cbd",
   "metadata": {},
   "source": [
    "### Loading and Filtering Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24575a4f-730b-47b2-ae98-e73794b3cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth data and filter\n",
    "def load_and_filter_hab_events(csv_file='habsos_20240430.csv'):\n",
    "    print(\"Loading ground truth data\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['SAMPLE_DATE'] = pd.to_datetime(df['SAMPLE_DATE'])\n",
    "    \n",
    "    # filter for Karenia brevis\n",
    "    kb_data = df[df['SPECIES'] == 'brevis'].copy()\n",
    "    kb_clean = kb_data.dropna(subset=['LATITUDE', 'LONGITUDE', 'SAMPLE_DATE', 'CELLCOUNT']).copy()\n",
    "    \n",
    "    # positive HAB when Karenia brevis > 50,000 cells/L\n",
    "    # negative HAB when Karenia brevis = 0 cells/L\n",
    "    positive_events = kb_clean[kb_clean['CELLCOUNT'] > 50000].copy()\n",
    "    positive_events['HAB_EVENT'] = 1\n",
    "    \n",
    "    negative_events = kb_clean[kb_clean['CELLCOUNT'] == 0].copy()\n",
    "    negative_events['HAB_EVENT'] = 0\n",
    "    \n",
    "    hab_events = pd.concat([positive_events, negative_events], ignore_index=True)\n",
    "    \n",
    "    # gulf of Mexico area\n",
    "    gulf_events = hab_events[\n",
    "        (hab_events['LATITUDE'] >= GULF_BOUNDS['lat_min']) &\n",
    "        (hab_events['LATITUDE'] <= GULF_BOUNDS['lat_max']) &\n",
    "        (hab_events['LONGITUDE'] >= GULF_BOUNDS['lon_min']) &\n",
    "        (hab_events['LONGITUDE'] <= GULF_BOUNDS['lon_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # filter closer to MODIS date (2003-2018)\n",
    "    modis_start = datetime(2003, 1, 1)\n",
    "    modis_end = datetime(2018, 12, 31)\n",
    "    final_events = gulf_events[\n",
    "        (gulf_events['SAMPLE_DATE'] >= modis_start) &\n",
    "        (gulf_events['SAMPLE_DATE'] <= modis_end)\n",
    "    ].copy()\n",
    "    \n",
    "    # create a UID \n",
    "    final_events['STABLE_EVENT_ID'] = (\n",
    "        final_events['SAMPLE_DATE'].dt.strftime('%Y%m%d') + '_' +\n",
    "        final_events['LATITUDE'].round(4).astype(str) + '_' +\n",
    "        final_events['LONGITUDE'].round(4).astype(str) + '_' +\n",
    "        final_events['CELLCOUNT'].astype(int).astype(str)\n",
    "    )\n",
    "    \n",
    "    # remove dupes based on ID\n",
    "    final_events = final_events.drop_duplicates(subset=['STABLE_EVENT_ID']).copy()\n",
    "    \n",
    "    # sort by date\n",
    "    final_events = final_events.sort_values('SAMPLE_DATE').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Total HAB events: {len(final_events):,}\")\n",
    "    print(f\"Positive: {len(final_events[final_events['HAB_EVENT'] == 1]):,}\")\n",
    "    print(f\"Negative: {len(final_events[final_events['HAB_EVENT'] == 0]):,}\")\n",
    "    print(f\"Dates range: {final_events['SAMPLE_DATE'].min()} to {final_events['SAMPLE_DATE'].max()}\")\n",
    "    \n",
    "    return final_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61cc6b-6e65-4389-b1df-b4b427710150",
   "metadata": {},
   "source": [
    "### Create sample events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71c171-3dbb-4190-b691-37c7ba44c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_events(events_df, n_events=10):\n",
    "    print(f\"\\nCreating sample of {n_events} events\")\n",
    "    \n",
    "    # try use recent years 2015-2018\n",
    "    recent_events = events_df[events_df['SAMPLE_DATE'].dt.year >= 2015].copy()\n",
    "    \n",
    "    # class balance\n",
    "    n_positive = min(n_events // 2, len(recent_events[recent_events['HAB_EVENT'] == 1]))\n",
    "    n_negative = min(n_events // 2, len(recent_events[recent_events['HAB_EVENT'] == 0]))\n",
    "    \n",
    "    # random state for reruns\n",
    "    positive_sample = recent_events[recent_events['HAB_EVENT'] == 1].sample(\n",
    "        n=n_positive, random_state=42\n",
    "    )\n",
    "    negative_sample = recent_events[recent_events['HAB_EVENT'] == 0].sample(\n",
    "        n=n_negative, random_state=42\n",
    "    )\n",
    "    \n",
    "    mvp_sample = pd.concat([positive_sample, negative_sample], ignore_index=True)\n",
    "    mvp_sample = mvp_sample.sort_values('SAMPLE_DATE').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"MVP sample created: {len(mvp_sample)} events\")\n",
    "    print(f\"  Positive (HAB): {len(mvp_sample[mvp_sample['HAB_EVENT'] == 1])}\")\n",
    "    print(f\"  Negative (No HAB): {len(mvp_sample[mvp_sample['HAB_EVENT'] == 0])}\")\n",
    "    print(f\"  Date range: {mvp_sample['SAMPLE_DATE'].min()} to {mvp_sample['SAMPLE_DATE'].max()}\")\n",
    "    \n",
    "    return mvp_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950b5b6-148e-4e9a-998d-7414c6af4753",
   "metadata": {},
   "source": [
    "### Modis L2 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9b60d-b2ad-4bc3-93a2-3effdffb6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MODIS L2 Ocean Color data\n",
    "def search_modis_l2_data(date, spatial_bounds):\n",
    "    bbox = (\n",
    "        spatial_bounds['lon_min'], spatial_bounds['lat_min'],\n",
    "        spatial_bounds['lon_max'], spatial_bounds['lat_max']\n",
    "    )\n",
    "    try:\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name='MODISA_L2_OC',\n",
    "            temporal=(date.strftime('%Y-%m-%d'), date.strftime('%Y-%m-%d')),\n",
    "            bounding_box=bbox\n",
    "        )\n",
    "        return granules\n",
    "    except Exception as e:\n",
    "        print(f\"Search error for {date}: {e}\")\n",
    "        return []\n",
    "\n",
    "#  download satelite granule if not cached\n",
    "def download_and_cache_granule(granule, raw_dir):\n",
    "    granule_name = granule['umm']['GranuleUR']\n",
    "    cached_file = raw_dir / f\"{granule_name}.nc\"\n",
    "    \n",
    "    if cached_file.exists():\n",
    "        print(f\"- using cached file: {cached_file.name[:50]}\")\n",
    "        return str(cached_file)\n",
    "    \n",
    "    try:\n",
    "        print(f\" - downloading new file\")\n",
    "        files = earthaccess.download([granule], local_path=str(raw_dir))\n",
    "        if files and len(files) > 0:\n",
    "            downloaded_file = Path(files[0])\n",
    "            if downloaded_file != cached_file:\n",
    "                downloaded_file.rename(cached_file)\n",
    "            return str(cached_file)\n",
    "    except Exception as e:\n",
    "        print(f\" - download failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# get the modalities\n",
    "def extract_modality_from_l2(file_path, modality, spatial_bounds, transformer_to_utm, transformer_from_utm):\n",
    "    try:\n",
    "        with nc.Dataset(file_path, 'r') as ds:\n",
    "            # check if required groups exist\n",
    "            if 'geophysical_data' not in ds.groups or 'navigation_data' not in ds.groups:\n",
    "                print(f\" missing required groups in {Path(file_path).name}\")\n",
    "                return None\n",
    "\n",
    "            geo_data = ds.groups['geophysical_data']\n",
    "            nav_data = ds.groups['navigation_data']\n",
    "\n",
    "            # get coords\n",
    "            lats = nav_data.variables['latitude'][:]\n",
    "            lons = nav_data.variables['longitude'][:]\n",
    "\n",
    "            # spatial filtering \n",
    "            lat_mask = (lats >= spatial_bounds['lat_min']) & (lats <= spatial_bounds['lat_max'])\n",
    "            lon_mask = (lons >= spatial_bounds['lon_min']) & (lons <= spatial_bounds['lon_max'])\n",
    "            spatial_mask = lat_mask & lon_mask\n",
    "\n",
    "            if not np.any(spatial_mask):\n",
    "                print(f\" - no data in spatial bounds\")\n",
    "                return None\n",
    "\n",
    "            # get modality data\n",
    "            if modality not in geo_data.variables:\n",
    "                print(f\" - {modality} not found in file\")\n",
    "                return None\n",
    "                \n",
    "            mod_data = geo_data.variables[modality][:]\n",
    "            if mod_data.shape != lats.shape:\n",
    "                print(f\" - {modality} shape mismatch\")\n",
    "                return None\n",
    "            \n",
    "            # apply spatial mask\n",
    "            lats_roi = lats[spatial_mask]\n",
    "            lons_roi = lons[spatial_mask]\n",
    "            vals_roi = mod_data[spatial_mask]\n",
    "            \n",
    "            # filter valid values \n",
    "            if hasattr(vals_roi, 'mask'):\n",
    "                valid_mask = ~vals_roi.mask\n",
    "            else:\n",
    "                valid_mask = np.isfinite(vals_roi)\n",
    "\n",
    "            # modality-specific filtering like habnet\n",
    "            if modality == 'chlor_a':\n",
    "                valid_mask = valid_mask & (vals_roi > 0) & (vals_roi < 1000) \n",
    "            elif modality.startswith('Rrs_'):\n",
    "                valid_mask = valid_mask & (vals_roi > 0) & (vals_roi < 1.0)\n",
    "            elif modality == 'par':\n",
    "                valid_mask = valid_mask & (vals_roi > 0)\n",
    "\n",
    "            if not np.any(valid_mask):\n",
    "                print(f\" - no valid {modality} data after filtering\")\n",
    "                return None\n",
    "            \n",
    "            # keep only valid data\n",
    "            final_lats = lats_roi[valid_mask]\n",
    "            final_lons = lons_roi[valid_mask]\n",
    "            final_vals = vals_roi[valid_mask]\n",
    "            \n",
    "            # reproject to utm like habnet in getData.m\n",
    "            utm_x, utm_y = transformer_to_utm.transform(final_lons, final_lats)\n",
    "            \n",
    "            print(f\" - {modality}: {len(final_vals)} points, \"\n",
    "                  f\"range {np.min(final_vals):.4f}-{np.max(final_vals):.4f}\")\n",
    "\n",
    "            return {\n",
    "                'lats': final_lats,\n",
    "                'lons': final_lons, \n",
    "                'utm_x': utm_x,\n",
    "                'utm_y': utm_y,\n",
    "                'values': final_vals\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - error with {Path(file_path).name}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414260fe-6ae8-452f-9bce-6d8a30aeb83e",
   "metadata": {},
   "source": [
    "### Reprojecting to Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07a2ec-1471-4b27-858b-ef2c66cb00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject to regular grid\n",
    "def reproject_to_regular_grid(utm_x, utm_y, values, center_utm_x, center_utm_y, config):\n",
    "    # define the projected coordinate ROI \n",
    "    half_extent = config['spatial_extent_km'] * 1000 / 2  # convert km to meters\n",
    "    west_utm = center_utm_x - half_extent\n",
    "    east_utm = center_utm_x + half_extent  \n",
    "    south_utm = center_utm_y - half_extent\n",
    "    north_utm = center_utm_y + half_extent\n",
    "    \n",
    "    # create regular grid \n",
    "    grid_size = config['spatial_extent_km'] // config['spatial_resolution_km']  # 100/2 = 50\n",
    "    resolution_m = config['spatial_resolution_km'] * 1000  # 2000m\n",
    "    \n",
    "    # habnet's affine transform\n",
    "    x_coords = np.linspace(west_utm + resolution_m/2, east_utm - resolution_m/2, grid_size)\n",
    "    y_coords = np.linspace(south_utm + resolution_m/2, north_utm - resolution_m/2, grid_size)\n",
    "    target_x_grid, target_y_grid = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "    # get source points for interpolation\n",
    "    source_points = np.column_stack([utm_x, utm_y])\n",
    "    target_points = np.column_stack([target_x_grid.ravel(), target_y_grid.ravel()])\n",
    "    \n",
    "    # need at least 4 points for triangulation\n",
    "    if len(values) < 4:\n",
    "        return np.full((grid_size, grid_size), np.nan)\n",
    "\n",
    "    try:\n",
    "        # interpolate to grid with linear \n",
    "        interpolated = griddata(\n",
    "            source_points, values, target_points,\n",
    "            method='linear', fill_value=np.nan\n",
    "        )\n",
    "        \n",
    "        # fill leftover NaNs with nearest neighbor if possible\n",
    "        if np.any(np.isnan(interpolated)) and len(values) >= 1:\n",
    "            interpolated_nn = griddata(\n",
    "                source_points, values, target_points,\n",
    "                method='nearest', fill_value=np.nan\n",
    "            )\n",
    "            nan_mask = np.isnan(interpolated)\n",
    "            interpolated[nan_mask] = interpolated_nn[nan_mask]\n",
    "\n",
    "        gridded_data = interpolated.reshape(target_x_grid.shape)\n",
    "        return gridded_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"- interpolation failed: {e}\")\n",
    "        return np.full((grid_size, grid_size), np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d9e67-7647-4901-a0c4-c8bce4f7d8cf",
   "metadata": {},
   "source": [
    "### dataCube Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6f173-f54c-4e34-bc83-09532551269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacube pipeline \n",
    "class HABNetDatacubeGenerator:\n",
    "    def __init__(self, raw_dir, processed_dir, config=DATACUBE_CONFIG):\n",
    "        self.raw_dir = Path(raw_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.config = config\n",
    "        self.search_cache = {}\n",
    "\n",
    "    # check if datacube already exists        \n",
    "    def check_existing_datacube(self, stable_event_id):\n",
    "        output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.h5\"\n",
    "        return output_file.exists()\n",
    "\n",
    "    # check if we already have the nc file\n",
    "    def cached_search_modis_data(self, date, spatial_bounds):\n",
    "        cache_key = f\"{date.strftime('%Y-%m-%d')}_{spatial_bounds['lat_min']:.2f}_{spatial_bounds['lon_min']:.2f}\"\n",
    "        \n",
    "        if cache_key in self.search_cache:\n",
    "            return self.search_cache[cache_key]\n",
    "        \n",
    "        granules = search_modis_l2_data(date, spatial_bounds)\n",
    "        self.search_cache[cache_key] = granules\n",
    "        return granules\n",
    "        \n",
    "    # create new datacube\n",
    "    def generate_datacube_for_event(self, event_row):\n",
    "        stable_event_id = event_row['STABLE_EVENT_ID']\n",
    "        \n",
    "        # skip if already processed\n",
    "        if self.check_existing_datacube(stable_event_id):\n",
    "            output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.h5\"\n",
    "            print(f\"\\nDatacube for event {stable_event_id} already exists, skipping\")\n",
    "            return output_file\n",
    "\n",
    "        event_date = event_row['SAMPLE_DATE']\n",
    "        event_lat = event_row['LATITUDE']\n",
    "        event_lon = event_row['LONGITUDE']\n",
    "        hab_label = event_row['HAB_EVENT']\n",
    "\n",
    "        print(f\"\\nGenerating datacube for event: {stable_event_id}\")\n",
    "        print(f\"Date: {event_date.strftime('%Y-%m-%d')}, Location: ({event_lat:.3f}, {event_lon:.3f})\")\n",
    "        print(f\"HAB Event: {hab_label}\")\n",
    "\n",
    "        # setup utm projection for this location\n",
    "        transformer_to_utm, transformer_from_utm, epsg_code = setup_utm_projection(event_lat, event_lon)\n",
    "        center_utm_x, center_utm_y = transformer_to_utm.transform(event_lon, event_lat)\n",
    "\n",
    "        # spatial and temporal bounds\n",
    "        spatial_bounds = calculate_spatial_bounds(event_lat, event_lon, self.config['spatial_extent_km'])\n",
    "        temporal_bounds = calculate_temporal_bounds(event_date, self.config['temporal_extent_days']-1)\n",
    "        total_days = (temporal_bounds['end_date'] - temporal_bounds['start_date']).days\n",
    "\n",
    "        # create h5 file\n",
    "        output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.h5\"\n",
    "        \n",
    "        # each modality gets its own group\n",
    "        modality_datacubes = {}\n",
    "        modality_points = {}  # store original points\n",
    "        modality_points_proj = {}  # store projected points\n",
    "        successful_days = 0\n",
    "\n",
    "        with h5py.File(output_file, 'w') as h5f:\n",
    "            # create ground truth group\n",
    "            gt_group = h5f.create_group('GroundTruth')\n",
    "            gt_group.attrs['thisLat'] = event_lat\n",
    "            gt_group.attrs['thisLon'] = event_lon\n",
    "            gt_group.attrs['thisCount'] = event_row['CELLCOUNT']\n",
    "            gt_group.attrs['HAB_EVENT'] = hab_label\n",
    "            gt_group.attrs['dayEnd'] = event_date.timestamp()\n",
    "            gt_group.attrs['dayStart'] = temporal_bounds['start_date'].timestamp()\n",
    "            gt_group.attrs['resolution'] = self.config['spatial_resolution_km'] * 1000  # in meters\n",
    "            gt_group.attrs['distance1'] = self.config['spatial_extent_km'] * 1000  # in meters\n",
    "            gt_group.attrs['projection'] = f'utm {epsg_code}'\n",
    "            \n",
    "            # store modality names\n",
    "            modality_names = [name.encode('utf-8') for name in HABNET_MODIS_AQUA_MODALITIES]\n",
    "            h5f.create_dataset('Modnames', data=modality_names)\n",
    "\n",
    "            # process each day in time window\n",
    "            for day_idx in range(total_days):\n",
    "                current_date = temporal_bounds['start_date'] + timedelta(days=day_idx)\n",
    "                print(f\"  Day {day_idx+1}/{self.config['temporal_extent_days']} ({current_date.strftime('%Y-%m-%d')}):\", end=\" \")\n",
    "\n",
    "                # search for modis data\n",
    "                granules = self.cached_search_modis_data(current_date, spatial_bounds)\n",
    "                if not granules:\n",
    "                    print(f\"No data found\")\n",
    "                    continue\n",
    "\n",
    "                # download and process first granule\n",
    "                file_path = download_and_cache_granule(granules[0], self.raw_dir)\n",
    "                if not file_path:\n",
    "                    print(f\"Download failed\")\n",
    "                    continue\n",
    "\n",
    "                # process all modalities from this granule\n",
    "                day_success = False\n",
    "                for modality in HABNET_MODIS_AQUA_MODALITIES:\n",
    "                    daily_data = extract_modality_from_l2(\n",
    "                        file_path, modality, spatial_bounds, \n",
    "                        transformer_to_utm, transformer_from_utm\n",
    "                    )\n",
    "                    \n",
    "                    if daily_data and len(daily_data['values']) > 0:\n",
    "                        # create modality group if it doesn't exist\n",
    "                        if modality not in modality_datacubes:\n",
    "                            modality_datacubes[modality] = np.full(\n",
    "                                (self.config['spatial_extent_km'] // self.config['spatial_resolution_km'],\n",
    "                                 self.config['spatial_extent_km'] // self.config['spatial_resolution_km'],\n",
    "                                 self.config['temporal_extent_days']), np.nan\n",
    "                            )\n",
    "                            modality_points[modality] = []\n",
    "                            modality_points_proj[modality] = []\n",
    "                        \n",
    "                        # reproject to regular grid\n",
    "                        gridded_data = reproject_to_regular_grid(\n",
    "                            daily_data['utm_x'], daily_data['utm_y'], daily_data['values'],\n",
    "                            center_utm_x, center_utm_y, self.config\n",
    "                        )\n",
    "                        modality_datacubes[modality][:, :, day_idx] = gridded_data\n",
    "                        \n",
    "                        # store points with time delta\n",
    "                        time_delta = day_idx  # days before event\n",
    "                        points_with_time = np.column_stack([\n",
    "                            daily_data['lats'], daily_data['lons'], daily_data['values'],\n",
    "                            np.full(len(daily_data['values']), time_delta)\n",
    "                        ])\n",
    "                        points_proj_with_time = np.column_stack([\n",
    "                            daily_data['utm_x'], daily_data['utm_y'], daily_data['values'],\n",
    "                            np.full(len(daily_data['values']), time_delta)\n",
    "                        ])\n",
    "                        \n",
    "                        modality_points[modality].append(points_with_time)\n",
    "                        modality_points_proj[modality].append(points_proj_with_time)\n",
    "                        \n",
    "                        day_success = True\n",
    "\n",
    "                if day_success:\n",
    "                    successful_days += 1\n",
    "                    print(f\"SUCCESS: processed {len(HABNET_MODIS_AQUA_MODALITIES)} modalities\")\n",
    "                else:\n",
    "                    print(f\"No valid data for any modality\")\n",
    "\n",
    "            # save all modality data to h5 file\n",
    "            for modality in modality_datacubes:\n",
    "                mod_group = h5f.create_group(modality)\n",
    "                mod_group.create_dataset('Ims', data=modality_datacubes[modality])\n",
    "                \n",
    "                # combine points from all days\n",
    "                if modality_points[modality]:\n",
    "                    all_points = np.vstack(modality_points[modality])\n",
    "                    all_points_proj = np.vstack(modality_points_proj[modality])\n",
    "                    mod_group.create_dataset('Points', data=all_points)\n",
    "                    mod_group.create_dataset('PointsProj', data=all_points_proj)\n",
    "\n",
    "        # check data completeness \n",
    "        data_completeness = successful_days / self.config['temporal_extent_days']\n",
    "        print(f\"\\nDatacube generation complete: {successful_days}/{self.config['temporal_extent_days']} days ({data_completeness:.1%})\")\n",
    "\n",
    "        if data_completeness >= 0.4: \n",
    "            print(f\"H5 datacube saved: {output_file}\")\n",
    "            return output_file\n",
    "        else:\n",
    "            print(f\"Insufficient data ({data_completeness:.1%}) - datacube not saved\")\n",
    "            if output_file.exists():\n",
    "                output_file.unlink()  # delete incomplete file\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfb7d4-becb-4e83-abfe-a8c9541786ee",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de08d7e-ad5a-4341-91f3-96062a8dcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_habnet_datacube_generation():\n",
    "    print(\"Datacube Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # setup directories\n",
    "    base_dir, raw_dir, processed_dir = setup_directories()\n",
    "    print(f\"Data directories created in: {base_dir}\")\n",
    "\n",
    "    # setup NASA auth\n",
    "    if not setup_nasa_earthdata():\n",
    "        print(\"NASA Earthdata auth needed\")\n",
    "        return None\n",
    "\n",
    "    # load and filter HAB events\n",
    "    try:\n",
    "        hab_events = load_and_filter_hab_events()\n",
    "    except FileNotFoundError:\n",
    "        print(\"HAB events CSV file not found. Please ensure 'habsos_20240430.csv' is available.\")\n",
    "        return None\n",
    "\n",
    "    # create sample \n",
    "    mvp_events = create_sample_events(hab_events, n_events=100)\n",
    "\n",
    "    # save events \n",
    "    mvp_file = base_dir / 'mvp_events.csv'\n",
    "    mvp_events.to_csv(mvp_file, index=False)\n",
    "    print(f\"Events saved to: {mvp_file}\")\n",
    "\n",
    "    # init datacube generator\n",
    "    generator = HABNetDatacubeGenerator(raw_dir, processed_dir)\n",
    "\n",
    "    print(f\"\\nDatacube Config:\")\n",
    "    print(f\"  Spatial: {generator.config['spatial_extent_km']}km x {generator.config['spatial_extent_km']}km\")\n",
    "    print(f\"  Temporal: {generator.config['temporal_extent_days']} days\")\n",
    "    print(f\"  Spatial resolution: {generator.config['spatial_resolution_km']}km (50x50 grid)\")\n",
    "    print(f\"  Modalities: {HABNET_MODIS_AQUA_MODALITIES}\")\n",
    "\n",
    "    # generate h5 datacubes for events\n",
    "    print(f\"\\nGenerating H5 datacubes for {len(mvp_events)} MVP events...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for idx, (event_idx, event) in enumerate(mvp_events.iterrows()):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing event {idx+1}/{len(mvp_events)}: ID {event_idx}\")\n",
    "        \n",
    "        event_start = datetime.now()\n",
    "        try:\n",
    "            output_file = generator.generate_datacube_for_event(event)\n",
    "            event_time = (datetime.now() - event_start).total_seconds() / 60\n",
    "            \n",
    "            results.append({\n",
    "                'event_id': event_idx,\n",
    "                'hab_label': event['HAB_EVENT'],\n",
    "                'output_file': output_file,\n",
    "                'success': output_file is not None,\n",
    "                'processing_time_min': event_time\n",
    "            })\n",
    "            \n",
    "            print(f\"Event completed in {event_time:.1f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            event_time = (datetime.now() - event_start).total_seconds() / 60\n",
    "            print(f\"Failed to process event {event_idx}: {e}\")\n",
    "            results.append({\n",
    "                'event_id': event_idx,\n",
    "                'hab_label': event['HAB_EVENT'],\n",
    "                'output_file': None,\n",
    "                'success': False,\n",
    "                'processing_time_min': event_time,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # summary\n",
    "    total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"HABNET DATACUBE GENERATION SUMMARY\")\n",
    "    print(f\"Total processing time: {total_time:.1f} minutes\")\n",
    "    print(f\"Average time per event: {total_time/len(results):.1f} minutes\")\n",
    "    print(f\"Total events processed: {len(results)}\")\n",
    "    print(f\"Successful: {len(successful_results)}\")\n",
    "    print(f\"Failed: {len(results) - len(successful_results)}\")\n",
    "\n",
    "    if successful_results:\n",
    "        print(f\"\\nSuccessful H5 datacubes saved in: {processed_dir}\")\n",
    "    return results, base_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting HABNet-Style Pipeline\")\n",
    "    print(f\"Configuration: {DATACUBE_CONFIG}\")\n",
    "    print(f\"MODIS-Aqua Modalities: {HABNET_MODIS_AQUA_MODALITIES}\")\n",
    "\n",
    "    results, data_dir = run_habnet_datacube_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
